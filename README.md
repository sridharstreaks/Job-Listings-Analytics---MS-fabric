# Job-Listings-Analytics---End-to-End Data Engineering Project MS-fabric

## Overview
This project represents a comprehensive data engineering initiative leveraging Microsoft Fabric technologies. The primary goal is to automate the extraction, transformation, and reporting of job data sourced from websites like Naukri and Linkedin. Emphasizing sustainability and automation, this project showcases expertise in Microsoft Azure Fabric, particularly following the recent achievement of passing the DP-600 Microsoft Fabric Analytics Engineer Associate exam.

![Pipeline Diagram](https://github.com/sridharstreaks/Job-Listings-Analytics---MS-fabric/blob/main/jobs_pipeline_snapshot_with_linkedin.png)

## Job Portals scraped
- **Linkedin**
- **Naukri**
- **Indeed** *(was previously open for scraping but now protected by Cloudflare)*

## Key Technologies Used
- **Microsoft Azure Services**: Azure Data Factory, Azure Data Lake Gen2, Azure Databricks (Pyspark Notebooks)
- **Other Tools**: Power BI for visualization, REST API for data retrieval
- **Programming Languages**: Python, SQL

## Objective
The project aims to create a scalable and automated data pipeline that retrieves job data from Naukri, processes it, and generates insightful reports using Azure's robust suite of data services. This README provides a detailed breakdown of the project's architecture, workflows, and implementation steps.

## Project Architecture
### 1. Data Sourcing
- **Source**: Job data is sourced from Naukri using a REST API call.
- **Source**: Job data is sourced from Linkedin using Pyspark Notebook. Since, Linkedin does output HTML instead of JSON
- **Flexibility**: Users can specify job profiles of interest via configurable parameters.
- **Data Storage**: JSON responses from the API are stored in Azure Data Lake Gen2, adhering to a lakehouse architecture for structured and unstructured data.

### 2. Data Transformation
- **Data Flow**: Azure Data Factory orchestrates the data pipeline.
- **Temporary Storage**: Job counts are extracted and temporarily stored in Azure Storage Tables for quick retrieval and processing efficiency.
- **Incremental Updates**: Existing data is overwritten during each pipeline run to optimize storage space and ensure freshness of information.

### 3. Data Processing
- **Pyspark Notebooks**: Utilized within Azure Databricks for complex data transformations and analysis.
- **Job Filtering**: Extracts job titles matching specified criteria from the stored JSON data.
- **Master Table**: Centralizes processed data into a structured format including job title, date listed, skills, location, and description, stored as a parquet table for optimal query performance.

### 4. Analytics and Reporting
- **Skills Analysis**: Pyspark notebooks calculate the top skills associated with each job title, contributing to a skills matrix for job market insights.
- **Salary and Experience Metrics**: Additional notebooks compute metrics such as minimum, maximum, and average salary, as well as required experience levels for each job title.

### 5. Automation and Scaling
- **Scheduled Execution**: The entire pipeline runs daily, ensuring the latest job data is continuously processed and updated.
- **Scalability**: Designed to handle increased data volume and accommodate additional job sites as the project evolves beyond its current beta phase.

## Current Status
The project is currently in beta, focusing on refining data extraction from Naukri. Future iterations will expand to include additional job sites and enhanced features before the Azure Fabric trial concludes.

## How to Use
To replicate this project:
1. **Azure Setup**:
   - Provision Azure services including Azure Data Factory, Azure Data Lake Gen2, and Azure Databricks.
   - Configure Azure Storage for temporary data handling.
   
2. **Data Extraction**:
   - Implement REST API integration to fetch job data from Naukri.
   - Customize API parameters to target specific job profiles.

3. **Data Processing**:
   - Develop and deploy Pyspark Notebooks within Azure Databricks.
   - Define job filtering and data transformation logic to align with project requirements.

4. **Analytics and Reporting**:
   - Utilize Power BI for creating visualizations based on processed data.
   - Integrate insights generated by Pyspark notebooks into dynamic reports for stakeholders.
  
5. **Necessary files along with detailed instructions will be added periodically. Hence await updates.**

## Conclusion
This project exemplifies an advanced data engineering solution using Microsoft Azure, highlighting expertise in Azure Fabric and demonstrating the seamless integration of various Azure services. By automating end-to-end data workflows, the project ensures efficiency, scalability, and actionable insights for informed decision-making in job market analysis.
